{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5m1SE3JgPm6J9wBvhqTM8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Unlearning of Federated Clusters\n","\n","![1](https://drive.google.com/uc?export=view&id=1VJ1xk3AMZA5fUmwUIhdkPQIgTy3fZ2ZB)\n","\n","- Unlearning of clusters at the client level via an intuitive application of $K$-means++ initialization without subsequent Lloyd's iterations. The advantage of using $K$-means++ is that the models do not need to be updated unless removed points were selected as initial centroids. At the server level, full $K$-means++  can be performed to improve the overall federated clustering performance.\n","- Enabling secure aggregation of sparse vectors via a novel approach termed \\emph{secure compressed multiset aggregation} (SCMA). The gist of SCMA is to encode quantized local data into information pertaining to the identifiers of quantization bins and their occupancies using specialized Reed-Solomon code evaluations, and followed by secure model aggregations.\n","- Ensuring privacy-accuracy-efficiency trade-offs through the introduction of a privacy criterion suitable for federated clustering and unlearning with simple communication protocols. In this context, we derive theoretical performance guarantees for the computational and communication complexity of model training and unlearning procedures.\n","- Compiling a collection of datasets for benchmarking unlearning of federated clusters, including two new datasets which contain anonymized cancer genomics and microbiome information that is subject to frequent unlearning requests. Experimental results reveal that our one-shot algorithm offers an average speed-up of roughly $84$x compared to complete retraining across seven datasets.\n","\n","## Machine Unlearning via Specialized Seeding\n","\n","![a1](https://drive.google.com/uc?export=view&id=1Jo-HsVnk_kVowUjUYaK-xkNlp5RhWGE-)\n","\n","### Performance Analysis\n","\n","Since the centroids chosen through $K$-means++ initialization are true data points, the results returned by Algorithm 1 are guaranteed to contain no information about the data points that have been removed. To verify that Algorithm 1 is an exact unlearning method, we also need to check that $\\mathbf{C}^\\prime$ is probabilistically equivalent to the models generated by rerunning the $K$-means++ initialization process on $\\mathcal{X}^\\prime$, the set of point remaining after removal. \n","\n","Suppose that one would like to unlearn the data point $x_i$ from the clustering model represented by an initial centroid set $\\mathbf{C}$. For the first  chosen centroid $c_1$ in $\\mathbf{C}$, there are only two possibilities: $c_1=x_i$ and $c_1\\neq x_i$. In the first case, we will have to re-run the initialization over $\\mathcal{X}^\\prime$, which is retraining the model. In the second case, since we know $c_1\\neq x_i$, the probability of choosing $c_1$ from $\\mathcal{X}$ as the first centroid becomes the conditional probability $\\mathbb{P}($choose $c_1$ from $\\mathcal{X}$ as the first centroid$|c_1\\neq x_i)=\\frac{1}{n-1}=\\mathbb{P}($choose $c_1$ from $\\mathcal{X}^\\prime$ as the first centroid$)$, where $n$ is the number of data points in $\\mathcal{X}$. Therefore, the first centroid $c_1^\\prime$ returned by Algorithm 1 can always be viewed as the first centroid obtained by re-running the initialization over $\\mathcal{X}^\\prime$. This can be generalized for $c_j\\;\\forall j\\in[K]$ and arbitrary $\\mathcal{X}_R$. Since Algorithm 1 is an exact unlearning method, we can have the performance guarantees in Lemma 1.\n","\n","- **Lemma 1.** For any set of data points $\\mathcal{X}$ and removal set $\\mathcal{X}_R$, assuming that the remaining dataset is $\\mathcal{X}^\\prime$ and the centroid set returned by Algorithm 1 is $\\mathbf{C}^\\prime$, we have $\\mathbb{E}(\\phi_c(X^\\prime;\\mathbf{C}^\\prime))\\leq 8(\\ln K+2)\\phi^*_c(X^\\prime)$.\n","\n","### Complexity Analysis\n","\n","We show next that Algorithm 1 supports removing $R$ random points in expected time $O(RK^2d)$ and $R$ adversarial points in expected time $O(RK^3\\epsilon_1\\epsilon_2d)$. In contrast, the corresponding time complexity of retraining is $O(nKRd)$. To analyze the removal time complexity of Algorithm 1, we first state two assumptions pertaining to optimal cluster sizes and outliers.\n","\n","- **Assumption 2.** Let $\\epsilon_1=\\frac{n}{K s_{\\min}}$ be a constant denoting what we refer to as **cluster size imbalance**, where $s_{\\min}$ equals the size of the smallest cluster in the optimal clustering; when $\\epsilon_1=1$, all clusters are of the same size $\\frac{n}{K}$.\n","\n","- **Assumption 3.** Assume that $\\epsilon_2\\geq 1$ is a fixed constant. An outlier $x_i$ in $\\mathcal{X}$ satisfies\n","$$\\|x_i-c_j^*\\|\\leq \\|x_i-c_k^*\\|, \\forall k\\in[K] \\quad \\text{ and } \\quad \\|x_i-c_j^*\\| > \\sqrt{\\frac{\\epsilon_2\\phi_c^*(\\mathcal{C}_j^*)}{|\\mathcal{C}_j^*|}}.$$\n","The assumption is not overly restrictive, as outliers are commonly defined as points that are well-separated from all \"regular\" clusters: the distance between an outlier and its closest centroid is larger than a scaled proxy for the empirical standard deviation of the average distance between the cluster points and that centroid. There also exist many different approaches for removing outliers for clustering purposes, which effectively reduce the probability of outliers to negligible values.\n","\n","Assumptions 2 and 3 can be used to derive an expression for the probability of $x_i\\in\\mathbf{C}$ when $x_i$ needs to be unlearned, which equals the probability of retraining with $\\mathcal{X}^\\prime$.\n","\n","- **Lemma 4.** Assume that the number of data points in $\\mathcal{X}$ is $n$ and the probability of the dataset containing at least one outlier is upper bounded by $O\\left({1}/{n}\\right)$. Let $\\mathbf{C}$ be the centroid set obtained by running $K$-means++ on $\\mathcal{X}$. For unlearning a data point $x_i \\in \\mathcal{X}$ we have that for random removal: $\\mathbb{P}(x_i\\in\\mathbf{C}) < O\\left({K}/{n}\\right)$; for adversarial removal: $\\mathbb{P}(x_i\\in\\mathbf{C}) < O \\left({K^2\\epsilon_1\\epsilon_2}/{n}\\right)$.\n","\n","ased on Algorithm 1, we need to reinitialize the centroids only when the point that requests removal lie in the original centroid set. We need $O(K)$ time to check if this is the case, and we need extra $O(nd)$ time to sample every new centroid due to the distance computation. Therefore, we arrive at an estimate of the expected removal time as shown in Theorem 5, which is independent of the problem size $n$. \n","\n","- **Theorem 5.** ssume that the number of data points in $\\mathcal{X}$ is $n$ and the probability of the dataset containing at least one outlier is upper bounded by $O\\left({1}/{n}\\right)$. Algorithm~\\ref{alg:mu_kpp} supports removing $R$ points with time $O(RK^2d)$ in expectation for random removals, and with time $O(RK^3\\epsilon_1\\epsilon_2d)$ in expectation for adversarial removals. The time complexity of complete retraining equals $O(nKRd)$.\n","\n","## Unlearning Federated Cluster\n","\n","![a2](https://drive.google.com/uc?export=view&id=1Zj09T5QnQZsL-PkNaVUzpjh1I-7aCVic)\n","\n","![a3](https://drive.google.com/uc?export=view&id=18r4Uqrm4Z_TPxHINOqzeTXIPiXAGvwnT)\n","\n","## SCMA\n","\n","![a4](https://drive.google.com/uc?export=view&id=1QSrGSx9yFWR1zhLCMAFiStUyXf1hv0yD)\n","\n","![2](https://drive.google.com/uc?export=view&id=13skOaGvy7CioNLhJ3welICt5x1SZWhWd)\n","\n","## Performance Analysis\n","\n","- **Definition 6.** Suppose that the local clusters at client $l$ are denoted by $\\mathcal{C}_k^{(l)},\\forall k\\in[K],l\\in[L]$, and that the clusters at the server are denoted by $\\mathcal{C}_k^s,\\forall k\\in[K]$. The global clustering equals $\\mathcal{P}_k=\\left\\{x_i^{(l)}| x_i^{(l)}\\in\\mathcal{C}_j^{(l)}, c_j^{(l)}\\in\\mathcal{C}_k^s,\\forall j\\in[K], l\\in[L]\\right\\},$\n","where $c_j^{(l)}$ is the centroid of $\\mathcal{C}_j^{(l)}$. Note that $(\\mathcal{P}_1,\\ldots,\\mathcal{P}_K)$ forms a partition of the entire dataset $\\mathcal{X}$, and the representative centroid for $\\mathcal{P}_k$ is defined as $c_{s,k}$ in $\\mathbf{C}_s$ returned by Algorithm 2.\n","\n","- **Lemma 7.** Suppose that the entire dataset across clients is denoted by $\\mathcal{X}$, and the set of server centroids returned by Algorithm~\\ref{alg:fl_nonsecure_clustering} is $\\mathbf{C}_s$. Then $\\mathbb{E}\\left(\\phi_f(X;\\mathbf{C}_s)\\right) < O(\\log^2 K)\\cdot \\phi_c^*(X).$\n","\n","- **Theorem 8.** Suppose that we performed uniform quantization with step size $\\gamma$ in Algorithm~\\ref{alg:fl_clustering}. Then we have\n","$\\mathbb{E}\\left(\\phi_f(X;\\mathbf{C}_s)\\right) < O(\\log^2 K)\\cdot \\phi_c^*(X) + O(nd\\gamma^2\\log K).$\n","\n","To make the second term in the upper bound a constant w.r.t. $n$, we can choose $\\gamma=\\Theta(1/\\sqrt{n})$, which is a good choice in practice offering good performance across different datasets.\n","\n","- **Corollary 9.** Denote by $X^\\prime$ the data set after removal, $\\mathbf{C}_s^\\prime$ the updated global centroids, and $R$ the number of removals. After unlearning, we have $\\mathbb{E}\\left(\\phi_f(X^\\prime;\\mathbf{C}_s^\\prime)\\right) < O(\\log^2 K)\\cdot \\phi_c^*(X^\\prime) + O((n-R)d\\gamma^2\\log K).$\n","\n","### Complexity Analysis\n","\n","**Removal time complexity.** We consider the removal requests in two federated clustering settings: removing $R$ points from only one client $l$ (cross-silo, single-client removal), and removing all data points from $R$ clients $l_1,\\ldots,l_R$ (cross-device, multi-client removal). For the case where multiple clients want to unlearn only a part of their data, the approach is similar to that of single-client removal and the corresponding analysis is based on a simple union bound.\n","\n","For single-client removal, we know from Theorem 5 that the expected removal time for client $l$ is $O(RK^2d)$ for random removal requests and $O(RK^3\\epsilon_1\\epsilon_2d)$ for adversarial removal requests. Other clients do not require additional computations, because our federated clustering algorithm is one-shot and thus their centroids will not be affected by the removal requests. On the other hand, the expected removal time for the server is upper bounded by $O(K^2LdT)$ for both types of removal requests, where $T$ is the maximum number of iterations of Lloyd's algorithm at the server before convergence.\n","\n","For multi-client removal, no client needs to perform additional computations. Since in this case we always re-run the full $K$-means++ clustering at the server, the expected removal time complexity at the server equals $O((L-R)K^2Td)$.\n","\n","**Communication complexity of SCMA at the client end.**\n","Since each $S^{(l)}_i$ can be represented by $\\lceil\\log p\\rceil$ bits, the information $\\{S_i^{(l)}\\}_{i\\in[2KL]}$ sent by each client $l$ can be represented by $2KL \\lceil\\log p\\rceil\\le\\max\\{2KL\\log n,2KLd\\log B\\}+1$ bits. Note that there are at most $\\sum_{k\\in[KL]}\\binom{B^d}{k}\\binom{n}{k-1}$ $q$-ary vectors of length $B^d$. Hence, the cost for communicating $q^{(l)}$ from the client to server $l$ is at least $\\log \\big(\\sum_{k\\in[KL]}\\binom{B^d}{k}\\binom{n}{k-1} \\big)=\\max\\{O(KL\\log n),O(KLd\\log B)\\}$ bits, which implies that our scheme is order-optimal w.r.t. the communication cost. Note that following standard practice in the area, we do not take into account the complexity of noise generation in secure model aggregation, as it can be done offline and independently of the Reed-Solomon encoding procedure. The computation of $S_i^{(l)}$ on client $l$ requires at most $O(K\\log i)$ multiplications over $\\mathbb{F}_p,$ $i\\in[2KL]$. The total computational complexity equals $O(K^2L\\log (KL))$ multiplication and addition operations over $\\mathbb{F}_p$.\n","\n","**Computational complexity of SCMA at the server end.**\n","The computational complexity at the server is dominated by the complexity of the Berlekamp-Massey decoding algorithm, factorizing the polynomial $g(x)$ over $\\mathbb{F}_p$, and solving the linear equations $S_i=\\sum_{l\\in[L]}S^{(l)}_i=\\sum_{j:q_j\\ne 0}q_j\\cdot j^{i-1}$ with known $j$, $q_j\\ne 0$. The complexity of Berlekamp-Massey decoding over $\\mathbb{F}_p$ is $O(K^2L^2)$. The complexity of factorizing a polynomial $g(x)$ over $\\mathbb{F}_p$ is $O((KL)^{1.5}\\log p+ KL\\log^2 p)$ operations over $\\mathbb{F}_p$. The complexity of solving for $S_i=\\sum_{l\\in[L]}S^{(l)}_i$ equals that of finding the inverse of a Vandermonde matrix, which takes $O(K^2L^2)$ operations over $\\mathbb{F}_p$. Hence, the total computational complexity at the server side is $\\max\\{O(K^2L^2), O((KL)^{1.5}\\log p+ KL\\log^2 p)\\}$ operations over $\\mathbb{F}_p$.\n","\n","## Experiments\n","\n","![t1](https://drive.google.com/uc?export=view&id=1AWjFc2ghqcsL_PRu31153oVBkkIdXMDE)\n","\n","![3](https://drive.google.com/uc?export=view&id=1c5DCagg_M_B2FzN_BWw8NGv-UMAfaMW-)\n","\n","\n","# References\n","- C. Pan, J. Sima, S. Prakash, V. Rana, and O. Milenkovic, Machine Unlearning of Federated Clusters. arXiv, 2022. doi: 10.48550/ARXIV.2210.16424. [[Paper](https://arxiv.org/abs/2210.16424)]"],"metadata":{"id":"ubC5alKC1jHL"}}]}